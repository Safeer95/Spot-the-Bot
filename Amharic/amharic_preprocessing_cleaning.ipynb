{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ac9719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bba66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of texts: 1337\n"
     ]
    }
   ],
   "source": [
    "with open('./amharic_all_text.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    dirty_corp = f.read().encode().decode('unicode_escape')\n",
    "\n",
    "dirty_corp = dirty_corp.split('^^^^^^')\n",
    "print(f'Num of texts: {len(dirty_corp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8142b84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"page_number\": 1, \"text\": \"ትምህርት 5\\n\\nየገባብ ክፍል:-  (3፳ ቆሮግቶስ 9;35-16)\\n\\nጥ2ቄ 3, በቁጥር 15 ላይ «እኀጺህ እግጺሆኀግልኻ ይህኻ አስጽፍምቅ»ቅ ሲል\\nሞኀ ማለቱ ነው?\\n\\nጥናቄ 2. በቁጥር 35 ላይ ፋትምክሕት» የሚለው ምኑገ ነው?\\n\\nጥዩቄ 5, በቁጥር 37 መሠረት ሐዋርደው ወገኻጌልኀ ይሰብክ የነበረው\\nበፈቃጽ ነበር ወይስ ፻ለፈቃጽሃ\\n\\nጥ5ናቄ 4, በቁጥር 38 ላይ ወኀግጌስኻ የመስበክ ጸመወዙ ምገኀጽነው ይላል?\\n\\nቁጥር 35;-- ክላይ ክዘረዘረቸው  መብቶች  በአጎገጹም እገኳ\\n#ልተጠቀመ እግጸሆነ ይመሰክረሪለል።። - በዚህ ገዜ በአእምሮው ዋመጣበት\\nምናልባት የቆሮኻቶስ ክርስቲፎኖች «ይህጎገ ሁሉ አሁኻ የሚናገረው ጸመወዝ\\nፈልጐ ነው ይሉኝ ይሆናለ!፤» የሚለው ግምት ነበር:  ይህም  እኀጻለሆነ\\nበክባጽ አነጋገር ዌሪጋግጥላቸዋል።- ማም ትምክህቴኻ ክኻቱ ከሚጸርግብኝ\\nሞት ይሻለናልና=»\\n\\nትምክህቱ ወግጌለኘኻ በነዓ መስበኩ ነው። ግኻ አሁኻ ፃሳቡኘ ቀይሮ\\nጸመጩዝ መቀበስ ቢጃምር ፳ ትምክህቱ ይቀረል።ሮ። ትምክህቱ የትሰቢት\\nሳይሆገኻ ጸመወዝ  በመቀበለ ሊመጣ የላነበረውኀ- ፃወገግጌልገ እኀቅፋት\\nበማስወገጹ ነበር።- በቆሮግቶስክ ቤተ ክርስቲፀ9ሓጣግ ውስጥ ሐዋርያውግኀ\\nየማይወጹ ሐሰተኛና ጸመወጩዝ ወጻጽ አእስተማሪዎች ተነሥተው ነበርና\\nየሐዋርፀው አቋም  ለእውነት በጣም አስፈላጊ ነበር።።-  እነዚህም ሐሰተኛ\\nከስተማሪዎች እርሱሞ ጸመመዝግኀ ተቀብሎ ክእነርሱ እኩፅ በምሞሆገ\\nእ፲፲፲ጻያሳጣቸው ይፈልጉ ነበር፲ (12፳ ቆሮ,33:32):\\n\\nጥ፻ደቄ 5, በአገጻግጽ ቤተ ክርስቲሄኻ ለጸመወዝ ብቻ ብለው የሚዩገለግሉ\\nመሪዎች ከሉ\"ኾ\"  ይህ ዓይነት ፃሳብ እገጹት ለወግጌልና ለቤተ\\nክርስቲ፻ግ እግቅፋት ሊሆገኻ ይችላል7\\n\\nቁጥር 36;- ወግጌለስካ ብሰብክ እግ'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_corp[1][1:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc5910",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6eee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner():\n",
    "    \"\"\"\n",
    "    Amharic text cleaner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.min_ascii = '1200'\n",
    "        self.max_ascii = '137F'\n",
    "        self.punctuation = string.punctuation + '።'+'፤'+'፥'+'፦'+'፧'+'፨'+'«'+'»'+'-'+':'+'፣'+'፡፡'+'*'+''\n",
    "        self.one_space_regex = r\"\\s((\\s)(\\s+)?)?\"\n",
    "        self.text = None\n",
    "\n",
    "        self.dict_punct = dict(zip(list(self.punctuation), np.repeat(' ', len(self.punctuation))))\n",
    "        \n",
    "    def remove_punct(self, text):\n",
    "        table = str.maketrans(self.dict_punct)\n",
    "        text = text.translate(table)\n",
    "        return text\n",
    "    \n",
    "    def remove_num(self, text):\n",
    "        num_pattern = r'[\\u1369-\\u137C]'\n",
    "        text = re.sub(num_pattern, ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_spaces(self, text):\n",
    "        try:\n",
    "            text = re.sub(self.one_space_regex, ' ', text)    \n",
    "            text = text if text[0] != ' ' else text[1:]\n",
    "            text = text if text[-1] != ' ' else text[:-1]\n",
    "            return text\n",
    "        \n",
    "        except IndexError as e:\n",
    "            return ''\n",
    "    \n",
    "    def is_am_token(self, token):\n",
    "        for ch in set(token):\n",
    "            if ord(ch) < int(self.min_ascii, 16) or ord(ch) > int(self.max_ascii, 16):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def remove_foreign_lang(self, text):\n",
    "        clean_text = ''\n",
    "        for token in text.split():\n",
    "            if self.is_am_token(token):\n",
    "                clean_text += ' ' + token\n",
    "        return clean_text[1:]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = self.remove_punct(text)\n",
    "        text = self.remove_num(text)\n",
    "        text = self.remove_spaces(text)\n",
    "        text = self.remove_foreign_lang(text)\n",
    "        text = self.remove_spaces(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df124a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a074fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [cleaner.clean_text(text) for text in dirty_corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28b71d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 28204888\n"
     ]
    }
   ],
   "source": [
    "# Initialize the word count to zero\n",
    "word_count = 0\n",
    "\n",
    "# Loop over each text in the corpus\n",
    "for text in cleaned_text:\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Increment the word count by the number of words in this text\n",
    "    word_count += len(words)\n",
    "\n",
    "print(f\"Total number of words: {word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440cda5",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "We will make two versions of the corpus - one without stopwords and the other with them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd9397",
   "metadata": {},
   "source": [
    "[Stopwords were taken from here](\"https://www.irit.fr/AmharicResources/stop-word-list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01db1169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Amharic stopwords were taken from this repository: https://github.com/Amharic-NLP/StopWords\n",
    "# Load the stopwords from the text file\n",
    "with open(\"amharic-stopword.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4afc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "414d2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each text in the cleaned corpus\n",
    "filtered_corpus = []\n",
    "for text in cleaned_text:\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    # Join the filtered words back into a single text\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    # Add the filtered text to the filtered corpus\n",
    "    filtered_corpus.append(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc07c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 25735662\n"
     ]
    }
   ],
   "source": [
    "#we will count words after removing stopwords\n",
    "# Initialize the word count to zero\n",
    "word_count = 0\n",
    "\n",
    "# Loop over each text in the corpus   \n",
    "for text in filtered_corpus:\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Increment the word count by the number of words in this text\n",
    "    word_count += len(words)\n",
    "\n",
    "print(f\"Total number of words: {word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f65cb283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ምህርት የገባብ ክፍል ቆሮግቶስ በቁጥር ላይ እኀጺህ እግጺሆኀግልኻ ይህኻ አስጽፍምቅ ቅ ሲል ሞኀ ማለቱ ነው ጥናቄ በቁጥር ላይ ፋትምክሕት የሚለው ምኑገ ነው '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_corpus[1][1:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bcaaaf",
   "metadata": {},
   "source": [
    "## Character Level Normalization\n",
    "Amharic has characters wich have the same sound that can be interchangably used.\n",
    "\n",
    "for example letters 'ሃ','ኅ','ኃ','ሐ','ሓ','ኻ','ሀ' have the same sound so we change them to 'ሀ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2825af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character level normalization was taken from here: \n",
    "#https://github.com/IsraelAbebe/An-Amharic-News-Text-classification-Dataset/blob/main/Amharic-News-Text-classification-Baseline.ipynb\n",
    "\n",
    "#method to normalize character level missmatch such as ጸሀይ and ፀሐይ\n",
    "def normalize_char_level_missmatch(input_token):\n",
    "    rep1=re.sub('[ሃኅኃሐሓኻ]','ሀ',input_token)\n",
    "    rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
    "    rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
    "    rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
    "    rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
    "    rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
    "    rep7=re.sub('[ሠ]','ሰ',rep6)\n",
    "    rep8=re.sub('[ሡ]','ሱ',rep7)\n",
    "    rep9=re.sub('[ሢ]','ሲ',rep8)\n",
    "    rep10=re.sub('[ሣ]','ሳ',rep9)\n",
    "    rep11=re.sub('[ሤ]','ሴ',rep10)\n",
    "    rep12=re.sub('[ሥ]','ስ',rep11)\n",
    "    rep13=re.sub('[ሦ]','ሶ',rep12)\n",
    "    rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
    "    rep15=re.sub('[ዑ]','ኡ',rep14)\n",
    "    rep16=re.sub('[ዒ]','ኢ',rep15)\n",
    "    rep17=re.sub('[ዔ]','ኤ',rep16)\n",
    "    rep18=re.sub('[ዕ]','እ',rep17)\n",
    "    rep19=re.sub('[ዖ]','ኦ',rep18)\n",
    "    rep20=re.sub('[ጸ]','ፀ',rep19)\n",
    "    rep21=re.sub('[ጹ]','ፁ',rep20)\n",
    "    rep22=re.sub('[ጺ]','ፂ',rep21)\n",
    "    rep23=re.sub('[ጻ]','ፃ',rep22)\n",
    "    rep24=re.sub('[ጼ]','ፄ',rep23)\n",
    "    rep25=re.sub('[ጽ]','ፅ',rep24)\n",
    "    rep26=re.sub('[ጾ]','ፆ',rep25)\n",
    "    #Normalizing words with Labialized Amharic characters such as በልቱዋል or  በልቱአል to  በልቷል  \n",
    "    rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
    "    rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
    "    rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
    "    rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
    "    rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
    "    rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
    "    rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
    "    rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
    "    rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
    "    rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
    "    rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
    "    rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
    "    rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
    "    rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
    "    rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
    "    rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
    "    rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
    "    rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
    "    rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
    "    rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
    "    rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
    "    rep48=re.sub('[ኵ]','ኩ',rep47) #ኩ can be also written as ኵ  \n",
    "    return rep48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72cab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each text in the list\n",
    "tokenized_texts = [word_tokenize(text) for text in filtered_corpus]\n",
    "normalized_texts = [[normalize_char_level_missmatch(token) for token in tokenized_text] for tokenized_text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2355e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corp = [' '.join(tokens) for tokens in normalized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "111bbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will save the file to use it for lemmatization in another place\n",
    "with open(\"amharic_normalized_cleaned_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    # Write each normalized text on a separate line\n",
    "    for text in cleaned_corp:\n",
    "        file.write(text + '^^^^^^')\n",
    "\n",
    "# Close the text file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fa879",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ec113",
   "metadata": {},
   "source": [
    "I used the lemmeizer in another place and just get beack the lemmatized text for further preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bb695",
   "metadata": {},
   "source": [
    "## Tokens replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1442371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we import the lemmatized text\n",
    "with open('./Amharic_normalized_lemmatized_text.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    cleaned_corp = f.read()\n",
    "cleaned_corp = cleaned_corp.split('\\n')\n",
    "cleaned_corp = [text for text in cleaned_corp if text != \"\"] #remove empty texts\n",
    "#there was some underscore signs after the lemmatizer, I need to remove it \n",
    "cleaned_corp = [text.replace(\"_\", \"\") for text in cleaned_corp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edd1df62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1320"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c776eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determiners were taken from here: https://en.wiktionary.org/wiki/Category:Amharic_determiners\n",
    "determiners = [\"ሁሉ\",\"ማንኛው\",\"ምንድር\", \"ምንድን\", \"ብዙ\",  \"እቺ\", \"እች\", \"እነዚህ\", \"እነዚያ\",\"ዚህ\", \"ዚች\", \"ዚያች\",\"የርሱ\", \"የርሷ\", \"የሱ\", \"የሷ\", \"የቱ\", \"የቲቱ\", \"የትኛው\", \"የቷ\", \"የነርሱ\", \"የነሱ\", \"የናንተ\", \"የኔ\", \"የኛ\", \"ያቺ\", \"ያች\", \"ያንተ\", \"ያንቺ\", \"ይህ\", \"ይህቺ\", \"ይህች\", \"ይቺ\", \"ይች\"]\n",
    "# pronouns were taken from here https://polyglotclub.com/wiki/Language/Amharic/Grammar/Personal-pronouns\n",
    "pronouns = [\"እኔ\", \"አንተ\", \"አንቺ\", \"እሱ\", \"እሷ\", \"እኛ\", \"እናንተ\", \"እነሱ\", \"ልጆችዎ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e745dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a pronouns and determiners replacement\n",
    "replacements = {\n",
    "    \"pronoun_token\": \"PRON\",\n",
    "    \"determiner_token\": \"DET\"\n",
    "}\n",
    "def replace_tokens(text):\n",
    "    replaced_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in pronouns:\n",
    "            replaced_text.append(replacements[\"pronoun_token\"])\n",
    "        elif word in determiners:\n",
    "            replaced_text.append(replacements[\"determiner_token\"])\n",
    "        else:\n",
    "            replaced_text.append(word)\n",
    "    return replaced_text\n",
    "\n",
    "replaced_texts = []\n",
    "for text in cleaned_corp:\n",
    "    replaced_text = replace_tokens(text)\n",
    "    replaced_texts.append(replaced_text)\n",
    "\n",
    "cleaned_corp = [' '.join(tokens) for tokens in replaced_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075666c",
   "metadata": {},
   "source": [
    "### Example of the text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcbbbc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: {\"page_number\": 1, \"text\": \"ትምህርት 5\n",
      "\n",
      "የገባብ ክፍል:-  (3፳ ቆሮግቶስ 9;35-16)\n",
      "\n",
      "ጥ2ቄ 3, በቁጥር 15 ላይ «እኀጺህ እግጺሆኀግልኻ ይህኻ አስጽፍምቅ»ቅ ሲል\n",
      "ሞኀ ማለቱ ነው?\n",
      "\n",
      "ጥናቄ 2. በቁጥር 35 ላይ ፋትምክሕት» የሚለው ምኑገ ነው?\n",
      "\n",
      "ጥዩቄ 5, በቁጥር 37 መሠረት ሐዋርደው ወገኻጌልኀ ይሰብክ የነበረው\n",
      "በፈቃጽ ነበር ወይስ ፻ለፈቃጽሃ\n",
      "\n",
      "ጥ5ናቄ 4, በቁጥር 38 ላይ ወኀግጌስኻ የመስበክ ጸመወዙ ምገኀጽነው ይላል?\n",
      "\n",
      "ቁጥር 35;-- ክላይ ክዘረዘረቸው  መብቶች  በአጎገጹም እገኳ\n",
      "#ልተጠቀመ እግጸሆነ ይመሰክረሪለል።። - በዚህ ገዜ በአእምሮው ዋመጣበት\n",
      "ምናልባት የቆሮኻቶስ ክርስቲፎኖች «ይህጎገ ሁ\n",
      "\n",
      "\n",
      "After: ምህርት የገባብ ክፍል ቆሮግቶስ በቁጥር ላይ እኀፂህ እግፂሆኀግልሀ ይህሀ አስፅፍምቅ ቅ ሲል ሞኀ ማለቱ ጥናቄ በቁጥር ላይ ፋትምክህት የሚለው ምኑገ ጥዩቄ በቁጥር መሰረት ሀዋርደው ወገሀጌልኀ ይሰብክ በፈቃፅ ነበር ለፈቃፅሀ በቁጥር ላይ ወኀግጌስሀ የመስበክ ፀመወዙ ምገኀፅነው ቁጥር ክላይ ክዘረዘረቸው መብቶች በአጎገፁም እገኳ ልተጠቀመ እግፀሆነ ይመሰክረሪለል ገዜ በአእምሮው ዋመጣበት የቆሮሀቶስ ክርስቲፎኖች ይህጎገ አሁሀ የሚናገረው ፀመወዝ ፈልጐ ይሉኝ ይሆናለ የሚለው ግምት ነበር ይህም እኀፃለሆነ በክባፅ አነጋገር ዌሪጋግጥላቸዋል ማም ትምክህቴሀ ክሀቱ ከሚፀርግብኝ ሞት ይሻለናልና ትምክህቱ ወግጌለኘሀ በነአ መስበኩ ግሀ አሁሀ ፃሳ\n"
     ]
    }
   ],
   "source": [
    "initial_text = dirty_corp[1][1:400]\n",
    "final_text = cleaned_corp[1][1:400]\n",
    "print(f'Before: {initial_text}')\n",
    "print('\\n')\n",
    "print(f'After: {final_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5052901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"amharic_lemmatized_cleaned_text.txt\", \"w\", encoding=\"utf-8\") as file:   \n",
    "    for text in cleaned_corp:\n",
    "        file.write(text + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9539504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
