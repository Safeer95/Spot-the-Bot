{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172a117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-bidi\n",
    "# pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdd60cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from hazm import word_tokenize\n",
    "from hazm import sent_tokenize\n",
    "from hazm import Normalizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3113c",
   "metadata": {},
   "source": [
    "### Learn about the text before starting to clean and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1af528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of texts: 1381\n"
     ]
    }
   ],
   "source": [
    "with open('./persian_combined_texts.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    dirty_corp = f.read()\n",
    "dirty_corp = dirty_corp.split('^^^^^^')\n",
    "dirty_corp = dirty_corp[:-1]\n",
    "print(f'Num of texts: {len(dirty_corp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc10dc3",
   "metadata": {},
   "source": [
    "Please note that the actuall number of texts here is 2000. It is just because when converting pdf files to  word files we sometimes combined more than one pdf file in one word file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc30c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شکیل شده ایم که خود با سرعتی سرسام آور در حرکت دورانی کاینات شریکند. آنهم در حالی است که اگر از فضای خالی موجود درکلٌ مواد کره زمین صرف نظر کنیم» اندازهٌ مجموع اين ارواح زندهٌ سیار از یک توپ فوتبال بزرگتر نخواهد بود.  گرچه خانه ازپای بست سست و ویران است» لاکن بنیانگذار آداب و سنن کهن آسیا یعنی زرتشت/بودا/ابراهیم خلیل بزرگمرد بی نظیر و بی مثال تاریخ عهد باستان بوده است.  دانش زرتشتیها از زرتشت تنها به اندازةٌ سر کوه یخی است که از اقیانوس تاریخ سر بر آورده است؛ پس باید دید دیگران از وی چه می گویند.   صفحه: 4  سیمای واقعی زرتشت تاریخی  (اتا آعطیناک الکوش)  دخمه گانوماته بردیه (زرتشت) در روستای سکاوند بخش هرسین کرمانشاهان  آب زنید راه راء هین که نسگار می رسد سس مژده دهید باغ را بوی بهار می رسد   صفحه: 5  معنی لفظی نام عجم و پیوند ناگسستنی تاریخی آن با جمشید و جم  می دانیم كلمةٌ عجم در زبان عرب به معنی کسی که دارای زبان غیر فصیح و لکنت داراست می باشد و از همینجاست که محمد بن جریر طبری به پیروی از افواه عامه بابک خرمدین را تحت این لقب یعنی دارای زبان لکنت دار آورده است» در صورتیکه بابک خرمدی\n"
     ]
    }
   ],
   "source": [
    "print(dirty_corp[1][1100:2100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b6875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 110037959 words.\n"
     ]
    }
   ],
   "source": [
    "#Count the number of words in the file\n",
    "num_words = sum(len(doc.split()) for doc in dirty_corp)\n",
    "#num_words\n",
    "print(\"The file contains\", num_words, \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ebbe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common words in the file are:\n",
      "و 4908774\n",
      ". 4586423\n",
      "در 2387302\n",
      "که 2309124\n",
      "از 2260401\n",
      "» 2034824\n",
      "به 1948159\n",
      ": 1833595\n",
      "را 1601446\n",
      "( 1213648\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter() \n",
    "for line in dirty_corp:\n",
    "    words = word_tokenize(line.strip())   # tokenize each line into words\n",
    "    word_counts.update(words)\n",
    "\n",
    "most_common_words = word_counts.most_common(10)\n",
    "print(\"The 10 most common words in the file are:\")\n",
    "for word, count in most_common_words:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0178544",
   "metadata": {},
   "source": [
    "Because the text is still unclean, we get the above most common words, which is a mess))\n",
    "\n",
    "So, we will clean the text and run this code later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16278c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "class TextCleaner():\n",
    "    \"\"\"\n",
    "    Persian text cleaner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.min_ascii = '0600'\n",
    "        self.max_ascii = '06FF'\n",
    "        self.punctuation = string.punctuation + '،' + '؛' + '؟' + '؛' + '۔' + '»' + '«' + '-'\n",
    "        self.one_space_regex = r\"\\s((\\s)(\\s+)?)?\"\n",
    "        self.text = None\n",
    "        self.dict_punct = dict(zip(list(self.punctuation), np.repeat(' ', len(self.punctuation))))\n",
    "    \n",
    "    def remove_punct(self, text):\n",
    "        # Remove patterns of the form \"number: صفحه\"\n",
    "        text = re.sub(r'صفحه\\s*:\\s*\\d+', '', text)\n",
    "        # Remove all other punctuation\n",
    "        table = str.maketrans(self.dict_punct)\n",
    "        text = text.translate(table)\n",
    "        return text\n",
    "    \n",
    "    def remove_num(self, text):\n",
    "        # Remove all numbers\n",
    "        num_pattern = r'[\\u06F0-\\u06F9]'\n",
    "        text = re.sub(num_pattern, ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_spaces(self, text):\n",
    "        # Remove extra spaces\n",
    "        try:\n",
    "            text = re.sub(self.one_space_regex, ' ', text)    \n",
    "            text = text if text[0] != ' ' else text[1:]\n",
    "            text = text if text[-1] != ' ' else text[:-1]\n",
    "            return text\n",
    "        \n",
    "        except IndexError as e:\n",
    "            return ''\n",
    "    \n",
    "    def is_fa_token(self, token):\n",
    "        # Check if token is in Persian language\n",
    "        for ch in set(token):\n",
    "            if ord(ch) < int(self.min_ascii, 16) or ord(ch) > int(self.max_ascii, 16):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def remove_foreign_lang(self, text):\n",
    "        # Remove non-Persian tokens\n",
    "        clean_text = ''\n",
    "        for token in text.split():\n",
    "            if self.is_fa_token(token):\n",
    "                clean_text += ' ' + token\n",
    "        return clean_text[1:]\n",
    "    \n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = self.remove_num(text)\n",
    "        text = self.remove_punct(text)\n",
    "        text = self.remove_spaces(text)\n",
    "        text = self.remove_foreign_lang(text)\n",
    "        text = self.remove_spaces(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c136ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = []\n",
    "\n",
    "text_cleaner = TextCleaner()\n",
    "\n",
    "for text in dirty_corp:\n",
    "    cleaned_text = text_cleaner.clean_text(text)\n",
    "    cleaned_texts.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec14510b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a79e16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cleaned_texts[1][1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf37c1",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797ff7d",
   "metadata": {},
   "source": [
    "[Stopwords were taken from here](\"https://github.com/mhbashari/awesome-persian-nlp-ir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdd673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stopwords from the text file\n",
    "with open(\"Persian-stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stop_words = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291d5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f292b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corp= []\n",
    "\n",
    "for text in cleaned_texts:\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    cleaned_corp.append(' '.join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cf202ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common words in the file are:\n",
      "سال 173800\n",
      "ایران 168728\n",
      "کار 142945\n",
      "دست 137521\n",
      "بن 132340\n",
      "تاریخ 110698\n",
      "کتاب 107934\n",
      "قرار 94694\n",
      "شاه 84279\n",
      "نظر 82769\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter() \n",
    "for line in cleaned_corp:\n",
    "    words = word_tokenize(line.strip())   # tokenize each line into words\n",
    "    word_counts.update(words)\n",
    "\n",
    "most_common_words = word_counts.most_common(10)\n",
    "print(\"The 10 most common words in the file are:\")\n",
    "for word, count in most_common_words:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca9307",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131bb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalized_texts = []\n",
    "for text in cleaned_corp: #change it to cleaned_corp after saving the coprus with stopwords\n",
    "    normalized_text= normalizer.normalize(text)\n",
    "    normalized_texts.append(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f5131",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118b689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "stemmed_lemmatized_texts = []\n",
    "\n",
    "for text in normalized_texts:\n",
    "    stemmed_text = [stemmer.stem(word) for word in text.split()]\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in stemmed_text]\n",
    "    stemmed_lemmatized_text = ' '.join(lemmatized_text)\n",
    "    stemmed_lemmatized_texts.append(stemmed_lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c51dd",
   "metadata": {},
   "source": [
    "### Replace pronouns, and determiners with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "368fb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "def replace_pronouns(text):\n",
    "    # Replace common Persian pronouns with a specific token\n",
    "    pronoun_mapping = {\n",
    "        'او': '<PRON>',\n",
    "        'اون': '<PRON>',\n",
    "        'اوه': '<PRON>',\n",
    "        'شما': '<PRON>',\n",
    "        'اوها': '<PRON>',\n",
    "\n",
    "        # Add more pronouns as needed\n",
    "    }\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    replaced_tokens = [pronoun_mapping.get(token, token) for token in tokens]\n",
    "    return ' '.join(replaced_tokens)\n",
    "\n",
    "def replace_determiners(text):\n",
    "    # Replace common Persian determiners with a specific token\n",
    "    determiner_mapping = {\n",
    "        'یک': '<DET>',\n",
    "        'یه': '<DET>',\n",
    "        'این': '<DET>',\n",
    "        'اون': '<DET>',\n",
    "        'همه': '<DET>',\n",
    "        'هر': '<DET>',\n",
    "        'همین': '<DET>',\n",
    "        \n",
    "        # Add more determiners as needed\n",
    "    }\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    replaced_tokens = [determiner_mapping.get(token, token) for token in tokens]\n",
    "    return ' '.join(replaced_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "135909d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = []\n",
    "for text in stemmed_lemmatized_texts:\n",
    "    cleaned_text = replace_pronouns(text)\n",
    "    cleaned_text = replace_determiners(cleaned_text)\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "\n",
    "# The cleaned_texts list now contains the cleaned and replaced texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85526a9",
   "metadata": {},
   "source": [
    "### An overview how the text looks before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb22d31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ای غلط نظامیان ذکر می‌کنند اما امروز عامل دیگری نیز به این دو عامل اضافه شده است. افکاز عمومی, گاهی افکار عمومی جنگ را می‌طلید.  بررسی تاریخ جنگ‌ها نشان می‌دهد پس از مدتی تخاصم لفظی و یا دست اندازی‌های محدود. شعله‌ی جنگ ناگهان بین دو يا چند ملت شعله‌ور و کشورء قاره و یا دنیایی به آتش کشیده می‌شود.  دلیل بروز نبردها ساده و گاهی کم اهمیت جلوه می‌کند اما واقعیت چیز دیگری‌ست. دلیل بروز نبرد مربوط به یک يا چند عامل ساده نیست. ده‌ها و یا شاید صدها علت کوچک و بزرگ باعث انباشت کینه‌ی ملت‌ها علیه یکدیگر می‌شود و آن زمان که وقت فوران خشم فرا می‌رسد. کسی نمی‌تواند مانع وقوع نبرد شود. همه چیز مانند بازی «دومینو» برهم می‌ريزد و حوادث زنجیروار رخ می‌دهد.  گم شدن یک ازن: ح ر کت یکت الاح جنگی بهوداخل خاک#کشور مقانل#شلنک یک گلوله: ترورایک شخصق ظهم! نحوه‌ی استفاده: از یک رودخانه‌ی مشترک, بندر مشت رک منبع آنرژی امشتر ک؛استناد به یک ادعای گهته ارضی, اعلام استقلال یک تژاد: تخت ستم. غلایق  ات نیز ووت روک اواته۱  نژادی» شوونیستی» راسیستی» ایدئولوژیکی و ده‌ها دلیل دیگر!  اما ملت‌های خسته از جنگ پس از آن که جن\n",
      "\n",
      "\n",
      "After: یکسره منف جنگ سبب رشد افزا اخلاق جنگ ادب گذ گذشتن مرد جنگ اولین مخالف جنگ طاق خاک سپردن مجموعه صد جنگ تاریخ هدف تدوین قالب مجموعه اطلاعات تمدن بشر خوانندگ داد # ده صد جنگ هزار جنگ تاریخ بشر انتخاب تو اذع مجموعه بود # باش ایراد فراوان نگارنده اعتقاد تلا بازشناس صد جنگ گزیده تاریخ کار مجموعه جمع آور تاریخ سیاس کشور حتا جغرافیا سیاس دنا تأثیر دوع ون بلط دوه عقو سا مک رخداد سبب مرگ ملل سرور رسانده مطالعه مقال نکات خوانندگ محتر گوشزده اطلاع موجود اندک نظر رسید # رس دلیل توضیح اندک ابتدا بضاع محدود علم نویسنده منابع مآخذ دسترس انتخاب عنصر انتخاب تأثیر تأریخ شد # شو نبرد دلاور جنگاور مدافع عوامل بعد نظر دلیل بررس دلایل جنگ تغییر جغرافیا سیاس منطقه وقوع جنگ پرداختن بررس دقیق موشکافانه نیازمند صرف وق کارگیر ورزیده نگارنده اذع امر بضاع خارج نکته مه محصول تحقیق جلد ارز آکادمیک فرا نیاز هزار خواننده دان آموز جوان وسایر اقشار جامعه پا ضرور یاد دوس بز رگوار مدیر انتشار هیرمند حمیدرضا باقرزاده گرام بدارم‌گر مساعد چاپ اثر میسر گ نبرد آشور ماد آشور جنگجو دنیا قد توجه منحصر پارس چین جنوب اروپا شمال شرق \n"
     ]
    }
   ],
   "source": [
    "initial_text = dirty_corp[2][1000:2000]\n",
    "final_text = cleaned_texts[2][1000:2000]\n",
    "print(f'Before: {initial_text}')\n",
    "print('\\n')\n",
    "print(f'After: {final_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7c42c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('persian_cleaned_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in cleaned_texts:\n",
    "        f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d7a62",
   "metadata": {},
   "source": [
    "We will save two versions of the corpus - one with stopwords and one without them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a26cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('persian_cleaned_corpus_with_stopwords.txt', 'w', encoding='utf-8') as f:\n",
    "#     for text in cleaned_texts:\n",
    "#         f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c0b51",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf95348",
   "metadata": {},
   "source": [
    "Here is another method for stemming that can be applied instead of hazm stemming. \n",
    "[PersianStemmer can be fouund here](\"https://github.com/htaghizadeh/PersianStemmer-Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "030d3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PersianStemmer\n",
    "# !pip install https://github.com/htaghizadeh/PersianStemmer-Python/archive/master.zip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e4d0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PersianStemmer import PersianStemmer \n",
    "\n",
    "# # Define the stemmer\n",
    "# stemmer = PersianStemmer()\n",
    "\n",
    "# # Stem each text in the filtered_texts list\n",
    "# stemmed_texts = []\n",
    "# for text in cleaned_corp:\n",
    "#     stemmed_text = ' '.join(stemmer.stem(w) for w in text.split())\n",
    "#     stemmed_texts.append(stemmed_text)\n",
    "\n",
    "# # Assign the stemmed texts to the original variable name\n",
    "# cleaned_corp = stemmed_texts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
