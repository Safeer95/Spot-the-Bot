{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-bidi\n",
    "# pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fdd60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from hazm import word_tokenize\n",
    "from hazm import sent_tokenize\n",
    "from hazm import Normalizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3113c",
   "metadata": {},
   "source": [
    "### Learn about the text before starting to clean and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c840c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of texts: 1382\n"
     ]
    }
   ],
   "source": [
    "with open('./persian_combined_text.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    dirty_corp = f.read()\n",
    "dirty_corp = dirty_corp.split('^^^^^^')\n",
    "print(f'Num of texts: {len(dirty_corp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc10dc3",
   "metadata": {},
   "source": [
    "Please note that the actuall number of texts here is 2000. It is just because when converting pdf files to  word files we sometimes combined more than one pdf file in one word file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc30c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "صفحه: 1  3.۱۳ ۷۷۷۷/۷۷/۰۳۳۰ در بو تس اگاه دا ود یبای نا یکی وی  برای دریافت کتابهای بیشتر به آدرس بالا مراجعه کنید  تمامی حقوق برای تاریخ ما محفوظ است ات ۵ و  کتابخانه مجزی «تزیخ ما» نخستین پایگاه دانلود کتابهاي تزیخی و مذهبی می باشد که زمان احداث آن به سال 1386 بز می گردد و تاکنون بسیزی از کتب تزیخی و مذهبی را به صورت الكترونيکي (۳0۴) بر روي دنياي مجزي منتشر نموده است.  ۱۵:5 ۷/۵۵۵ ۰ ۵[۱.001 ۳۵2۵۲۱۰۴۱۵01 : از۵ ۴۳ 7۲ م۵ ۲۲۵://۸۵۱۵۱.۱۲ :۷۷۶۵۵ ۵۵۲ ۵۸0۲655 ۴6۵۵۴  ۷۷۷۷۷۷۰۳۳۰۱۵۲۱۱۱ ۱۲   صفحه: 2  تاریخ و اساطیر تطبیقی ایران باستان )۱(  سیمای واقعی زرتشت تاریخی  به همراه معرفی چهرةٌ تاریخی انبیای کتب مقدس (تلاشی در پایان نهادن بر عهد اساطیر)  در مورد زرتشت و اوستا ق تاریخ اساطیری ایران  ۲۱۰۰۵ شهر لینشوپینگ. سوند  نفس باد صبا مشک فشان خواهد شد عالم پیر دگر باره جوان خواهد شد   صفحه: 3  لب مطلب پیام عیسی مسیح تاریخی یعنی یهودای جلیلی فرزند زیپورایی این بود که به جز خدای یکتا نپرستید و از جهانخواران اطاعت نکرده و بدیشان باج ندهید.  عنوان کتاب و يا کتابهای آسمانی که افراط و تفریط در متون آنها تباه\n"
     ]
    }
   ],
   "source": [
    "print(dirty_corp[1][1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49b6875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 110037959 words.\n"
     ]
    }
   ],
   "source": [
    "#Count the number of words in the file\n",
    "num_words = sum(len(doc.split()) for doc in dirty_corp)\n",
    "#num_words\n",
    "print(\"The file contains\", num_words, \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12ebbe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common words in the file are:\n",
      "و 4908774\n",
      ". 4586423\n",
      "در 2387302\n",
      "که 2309124\n",
      "از 2260401\n",
      "» 2034824\n",
      "به 1948159\n",
      ": 1833595\n",
      "را 1601446\n",
      "( 1213648\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter() \n",
    "for line in dirty_corp:\n",
    "    words = word_tokenize(line.strip())   # tokenize each line into words\n",
    "    word_counts.update(words)\n",
    "\n",
    "most_common_words = word_counts.most_common(10)\n",
    "print(\"The 10 most common words in the file are:\")\n",
    "for word, count in most_common_words:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0178544",
   "metadata": {},
   "source": [
    "Because the text is still unclean, we get the above most common words, which is a mess))\n",
    "\n",
    "So, we will clean the text and run this code later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16278c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "class TextCleaner():\n",
    "    \"\"\"\n",
    "    Persian text cleaner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.min_ascii = '0600'\n",
    "        self.max_ascii = '06FF'\n",
    "        self.punctuation = string.punctuation + '،' + '؛' + '؟' + '؛' + '۔' + '»' + '«' + '-'\n",
    "        self.one_space_regex = r\"\\s((\\s)(\\s+)?)?\"\n",
    "        self.text = None\n",
    "        self.dict_punct = dict(zip(list(self.punctuation), np.repeat(' ', len(self.punctuation))))\n",
    "    \n",
    "    def remove_punct(self, text):\n",
    "        # Remove patterns of the form \"number: صفحه\"\n",
    "        text = re.sub(r'صفحه\\s*:\\s*\\d+', '', text)\n",
    "        # Remove all other punctuation\n",
    "        table = str.maketrans(self.dict_punct)\n",
    "        text = text.translate(table)\n",
    "        return text\n",
    "    \n",
    "    def remove_num(self, text):\n",
    "        # Remove all numbers\n",
    "        num_pattern = r'[\\u06F0-\\u06F9]'\n",
    "        text = re.sub(num_pattern, ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_spaces(self, text):\n",
    "        # Remove extra spaces\n",
    "        try:\n",
    "            text = re.sub(self.one_space_regex, ' ', text)    \n",
    "            text = text if text[0] != ' ' else text[1:]\n",
    "            text = text if text[-1] != ' ' else text[:-1]\n",
    "            return text\n",
    "        \n",
    "        except IndexError as e:\n",
    "            return ''\n",
    "    \n",
    "    def is_fa_token(self, token):\n",
    "        # Check if token is in Persian language\n",
    "        for ch in set(token):\n",
    "            if ord(ch) < int(self.min_ascii, 16) or ord(ch) > int(self.max_ascii, 16):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def remove_foreign_lang(self, text):\n",
    "        # Remove non-Persian tokens\n",
    "        clean_text = ''\n",
    "        for token in text.split():\n",
    "            if self.is_fa_token(token):\n",
    "                clean_text += ' ' + token\n",
    "        return clean_text[1:]\n",
    "    \n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = self.remove_num(text)\n",
    "        text = self.remove_punct(text)\n",
    "        text = self.remove_spaces(text)\n",
    "        text = self.remove_foreign_lang(text)\n",
    "        text = self.remove_spaces(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68c136ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = []\n",
    "\n",
    "text_cleaner = TextCleaner()\n",
    "\n",
    "for text in dirty_corp:\n",
    "    cleaned_text = text_cleaner.clean_text(text)\n",
    "    cleaned_texts.append(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec14510b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a79e16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cleaned_texts[1][1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf37c1",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797ff7d",
   "metadata": {},
   "source": [
    "[Stopwords were taken from here](\"https://github.com/mhbashari/awesome-persian-nlp-ir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfdd673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stopwords from the text file\n",
    "with open(\"Persian-stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stop_words = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "291d5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f292b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corp= []\n",
    "\n",
    "for text in cleaned_texts:\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    cleaned_corp.append(' '.join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cf202ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common words in the file are:\n",
      "سال 173800\n",
      "ایران 168728\n",
      "کار 142945\n",
      "دست 137521\n",
      "بن 132340\n",
      "تاریخ 110698\n",
      "کتاب 107934\n",
      "قرار 94694\n",
      "شاه 84279\n",
      "نظر 82769\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter() \n",
    "for line in cleaned_corp:\n",
    "    words = word_tokenize(line.strip())   # tokenize each line into words\n",
    "    word_counts.update(words)\n",
    "\n",
    "most_common_words = word_counts.most_common(10)\n",
    "print(\"The 10 most common words in the file are:\")\n",
    "for word, count in most_common_words:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca9307",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "131bb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalized_texts = []\n",
    "for text in cleaned_corp:\n",
    "    normalized_text= normalizer.normalize(text)\n",
    "    normalized_texts.append(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f5131",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "118b689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "stemmed_lemmatized_texts = []\n",
    "\n",
    "for text in normalized_texts:\n",
    "    stemmed_text = [stemmer.stem(word) for word in text.split()]\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in stemmed_text]\n",
    "    stemmed_lemmatized_text = ' '.join(lemmatized_text)\n",
    "    stemmed_lemmatized_texts.append(stemmed_lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85526a9",
   "metadata": {},
   "source": [
    "### An overview how the text looks before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb22d31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: صفحه: 1  3.۱۳ ۷۷۷۷/۷۷/۰۳۳۰ در بو تس اگاه دا ود یبای نا یکی وی  برای دریافت کتابهای بیشتر به آدرس بالا مراجعه کنید  تمامی حقوق برای تاریخ ما محفوظ است ات ۵ و  کتابخانه مجزی «تزیخ ما» نخستین پایگاه دانلود کتابهاي تزیخی و مذهبی می باشد که زمان احداث آن به سال 1386 بز می گردد و تاکنون بسیزی از کتب تزیخی و مذهبی را به صورت الكترونيکي (۳0۴) بر روي دنياي مجزي منتشر نموده است.  ۱۵:5 ۷/۵۵۵ ۰ ۵[۱.001 ۳۵2۵۲۱۰۴۱۵01 : از۵ ۴۳ 7۲ م۵ ۲۲۵://۸۵۱۵۱.۱۲ :۷۷۶۵۵ ۵۵۲ ۵۸0۲655 ۴6۵۵۴  ۷۷۷۷۷۷۰۳۳۰۱۵۲۱۱۱ ۱۲   صفحه: 2  تاریخ و اساطیر تطبیقی ایران باستان )۱(  سیمای واقعی زرتشت تاریخی  به همراه معرفی چهرةٌ تاریخی انبیای کتب مقدس (تلاشی در پایان نهادن بر عهد اساطیر)  در مورد زرتشت و اوستا ق تاریخ اساطیری ایران  ۲۱۰۰۵ شهر لینشوپینگ. سوند  نفس باد صبا مشک فشان خواهد شد عالم پیر دگر باره جوان خواهد شد   صفحه: 3  لب مطلب پیام عیسی مسیح تاریخی یعنی یهودای جلیلی فرزند زیپورایی این بود که به جز خدای یکتا نپرستید و از جهانخواران اطاعت نکرده و بدیشان باج ندهید.  عنوان کتاب و يا کتابهای آسمانی که افراط و تفریط در متون آنها تباهی می آفریند» بی معنی است. وقتی که ما و زمین و خورشید مان خود آسمانی هستیم و از ذراتی از مادةٌ نور تشکیل شده ایم که خود با سرعتی سرسام آور در حرکت دورانی کاینات شریکند. آنهم در حالی است که اگر از فضای خالی موجود درکلٌ مواد کره زمین صرف نظر کنیم» اندازهٌ مجموع اين ارواح زندهٌ سیار از یک توپ فوتبال بزرگتر نخواهد بود.  گرچه خانه ازپای بست سست و ویران است» لاکن بنیانگذار آداب و سنن کهن آسیا یعنی زرتشت/بودا/ابراهیم خلیل بزرگمرد بی نظیر و بی مثال تاریخ عهد باستان بوده است.  دانش زرتشتیها از زرتشت تنها به اندازةٌ سر کوه یخی است که از اقیانوس تاریخ سر بر آورده است؛ پس باید دید دیگران از وی چه می گویند.   صفحه: 4  سیمای واقعی زرتشت تاریخی  (اتا آعطیناک الکوش)  دخمه گانوماته بردیه (زرتشت) در روستای سکاوند بخش هرسین کرمانشاهان  آب زنید راه راء هین که نسگار می رسد سس مژده دهید باغ را بوی بهار می رسد   صفحه: 5  معنی لفظی نام عجم و پیوند ناگسستنی تاریخی آن با جمشید و جم  می دانیم كلمةٌ عجم در زبان عرب به معنی کسی که دارای زبان غیر فصیح و لکنت داراست می باشد و از همینجاست که محمد بن جریر طبری به پیروی\n",
      "\n",
      "\n",
      "After: و تس اگاه ود یبا نا دریاف کتاب آدرس مراجعه حقوق تاریخ محفوظ  کتابخانه مجز تزیخ پایگاه دانلود کتاب تزیخ مذهب احداث سال بز بسیز کتب تزیخ مذهب الکترونیک رو دنیا مجز منتشر نموده تاریخ اساطیر تطبیق ایر باس سیما زر تاریخ همراه معرف چهرة تاریخ انبیا کتب مقدس تلاش پا نهادن عهد اساطیر زر اوستا تاریخ اساطیر ایر شهر لینشوپینگ سوند نفس باد صبا مشک ف عال پیر دگر جو مطلب پیا عیس مسیح تاریخ یهودا جلیل فرزند زیپورا خدا یکتا نپرستید جهانخوار اطاع بد باج داد#ده کتاب یا کتاب آسمان افراط تفریط متون تباه آفریند معن زمین خورشید آسمان ذرات مادة نور تشکیل سرعت سرسا آور حرک دوران کاین شریکند آنه حال فضا خال موجود درکل مواد کره زمین صرف نظر اندازه مجموع این ارواح زنده سیار توپ فوتبال بزرگ خانه ازپا بس سس ویر لاکن بنیانگذار آداب سنن کهن آسیا زر بودا ابراه خلیل بزرگمرد مثال تاریخ عهد باس دان زرتشت زر اندازة کوه یخ اقیانوس تاریخ دید سیما زر تاریخ اتا آعطیناک الکو دخمه گانوماته بردیه زر روستا سکاوند هرسین کرمانشاه آب زد#زن راء هین نسگار رسید#رس سس مژده باغ بو بهار رسید#رس معن لفظ عج پیوند ناگسستن تاریخ جمشید ج دان کلمة عج زب عرب معن دارا زب فصیح لکن همینجاس محمد بن جریر طبر پیرو افواه عامه بابک خرمدین لقب دارا زب لکن صورتیکه بابک خرمدین تنبورزن اعتقاد تناسخ فرزندکور کوراوغلو تخلص نموده ترانه غرا تشجیع لشکر خوانده ترانه روزگار درزب عاشیق آذر ترانه سرا خواه سلامت باق مانده کلمة عج قدیم دوره سیاد اعراب مسل میباشد مربوط دوره قبائل پراکنده عرب شرایط بود#باش توانست#توان لقب تحقیرآمیز سرور ایران داد#ده دید شکل معن اصل عج درزبان عرب وایران نگارنده تردید اصل این کلمه درزب عرب الج اف لا حرف تعریف شمس حرف ال صرف نمیشود زب عرب معن انجمن چونکه مبدان معن لفظ قبیله طبقهة زوخانیون قد یز ایخن مغ مچیس تور قزآن بزا تأیید دزست نظرهمچنین زرتشت گبر گور دردس معن جمع انجمن گرد آت دان کلمه درنا فرقة گور دانا سرود دین باق مانده کلمة عج دیگربه اساس ج جمشید اساطیرایران پیوند جم اژیدهاک ثان ضحاک آستیاگ مربوط میشده شخص ازقو مغ آذربایج قول کتسیاس منابع اوستا سپیتمه دانا سفید مقدس نامیده میشده کتسیاس سپیتمه درفهرس حکمران ماد اسپنداس ارمغ کننده خوشبخت اوستا معن لفظ اشاره آخرین فرمانروا ماد حکوم س وپنج سال قائل آست گاس صاحب وار\n"
     ]
    }
   ],
   "source": [
    "initial_text = dirty_corp[1][1:2000]\n",
    "final_text = stemmed_lemmatized_texts[1][1:2000]\n",
    "print(f'Before: {initial_text}')\n",
    "print('\\n')\n",
    "print(f'After: {final_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7c42c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('persian_cleaned_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in stemmed_lemmatized_texts:\n",
    "        f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c0b51",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf95348",
   "metadata": {},
   "source": [
    "Here is another method for stemming that can be applied instead of hazm stemming. \n",
    "[PersianStemmer can be fouund here](\"https://github.com/htaghizadeh/PersianStemmer-Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "030d3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PersianStemmer\n",
    "# !pip install https://github.com/htaghizadeh/PersianStemmer-Python/archive/master.zip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e4d0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PersianStemmer import PersianStemmer \n",
    "\n",
    "# # Define the stemmer\n",
    "# stemmer = PersianStemmer()\n",
    "\n",
    "# # Stem each text in the filtered_texts list\n",
    "# stemmed_texts = []\n",
    "# for text in cleaned_corp:\n",
    "#     stemmed_text = ' '.join(stemmer.stem(w) for w in text.split())\n",
    "#     stemmed_texts.append(stemmed_text)\n",
    "\n",
    "# # Assign the stemmed texts to the original variable name\n",
    "# cleaned_corp = stemmed_texts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
